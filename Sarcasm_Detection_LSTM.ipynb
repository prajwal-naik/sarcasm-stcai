{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snig/snap/jupyter/common/lib/python3.7/site-packages/joblib/_multiprocessing_helpers.py:45: UserWarning: [Errno 13] Permission denied.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import urllib\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import zipfile\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware  dirty step to get money  #staylight ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#sarcasm for #people who don't understand #diy...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@IminworkJeremy @medsingle #DailyMail readers ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@wilw Why do I get the feeling you like games?...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-@TeacherArthurG @rweingarten You probably jus...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  is_sarcastic\n",
       "0  Be aware  dirty step to get money  #staylight ...           0.0\n",
       "1  #sarcasm for #people who don't understand #diy...           0.0\n",
       "2  @IminworkJeremy @medsingle #DailyMail readers ...           0.0\n",
       "3  @wilw Why do I get the feeling you like games?...           0.0\n",
       "4  -@TeacherArthurG @rweingarten You probably jus...           0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "twitter_df = pd.concat([train,test])\n",
    "\n",
    "twitter_df['class'] = twitter_df['class'].replace(['irony','figurative','regular'],int('0'))\n",
    "twitter_df['class'] = twitter_df['class'].replace(['sarcasm'],int('1'))\n",
    "\n",
    "\n",
    "twitter_df = twitter_df.rename(columns={'tweets':'sentence','class':'is_sarcastic'})\n",
    "\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware  dirty step to get money  #staylight ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#sarcasm for #people who don't understand #diy...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#DailyMail readers being sensible as always ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do I get the feeling you like games? #sar...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-  You probably just missed the text. #sarcastic</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  is_sarcastic\n",
       "0  Be aware  dirty step to get money  #staylight ...           0.0\n",
       "1  #sarcasm for #people who don't understand #diy...           0.0\n",
       "2    #DailyMail readers being sensible as always ...           0.0\n",
       "3   Why do I get the feeling you like games? #sar...           0.0\n",
       "4   -  You probably just missed the text. #sarcastic           0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input dataframe\n",
    "def keep_uniques(data):\n",
    "    data = data.drop_duplicates('sentence')\n",
    "    return data\n",
    "\n",
    "# input cell (tweet)\n",
    "def remove_url(text):\n",
    "    text = str(text)\n",
    "    url = re.compile(r'[https|http]?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text) \n",
    "\n",
    "# input cell (tweet)\n",
    "def remove_emoji(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                   \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "# input cell (tweet)\n",
    "def remove_tag(text):\n",
    "    tag = re.compile(r\"(@[a-zA-Z0-9._]+)\")\n",
    "    return tag.sub(r'', text)\n",
    "\n",
    "# input cell (tweet)\n",
    "def remove_splsymbols(text):\n",
    "    sym = re.compile(r\"[$|%|^|&|*|=|;]+\")\n",
    "    return sym.sub(r'', text)\n",
    "\n",
    "twitter_df = keep_uniques(twitter_df)\n",
    "twitter_df['sentence'] = twitter_df['sentence'].apply(lambda x: remove_url(x))\n",
    "twitter_df['sentence'] = twitter_df['sentence'].apply(lambda x: remove_emoji(x))\n",
    "twitter_df['sentence'] = twitter_df['sentence'].apply(lambda x: remove_tag(x))\n",
    "twitter_df['sentence'] = twitter_df['sentence'].apply(lambda x: remove_splsymbols(x))\n",
    "\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Headlines Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(file):\n",
    "    for l in open(file,'r'):\n",
    "        yield json.loads(l)\n",
    "\n",
    "data = list(open_data('Sarcasm_Headlines_Dataset.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_with_stopwords = []\n",
    "label = []\n",
    "\n",
    "for line in data:\n",
    "    headline_with_stopwords.append(line['headline'])\n",
    "    label.append(line['is_sarcastic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
       " 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_with_stopwords[1], label[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = []\n",
    "for line in headline_with_stopwords:\n",
    "    temp = []\n",
    "    for word in line.split():\n",
    "        if word not in stopwords:\n",
    "            temp.append(word)\n",
    "    join = ' '.join([str(ele) for ele in temp])\n",
    "    headline.append(join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'roseanne' revival catches thorny political mood, better worse\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_json('Sarcasm_Headlines_Dataset.json',lines=True)\n",
    "data_df = data_df.rename(columns = {'headline': 'sentence'})\n",
    "data_df.drop('article_link',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8123</th>\n",
       "      <td>Why yes I will totally submit my photos to a s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8124</th>\n",
       "      <td>Test on a Saturday! Thank you uni! #sarcasm @ ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8125</th>\n",
       "      <td>Listening to 's Misery isn't at all disconcert...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8126</th>\n",
       "      <td>There you go being kind again #sarcasm #stand...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8127</th>\n",
       "      <td>I'm shocked that these refs in the tcu vs minn...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100108 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  is_sarcastic\n",
       "0     former versace store clerk sues over secret 'b...           0.0\n",
       "1     the 'roseanne' revival catches up to our thorn...           0.0\n",
       "2     mom starting to fear son's web series closest ...           1.0\n",
       "3     boehner just wants wife to listen, not come up...           1.0\n",
       "4     j.k. rowling wishes snape happy birthday in th...           0.0\n",
       "...                                                 ...           ...\n",
       "8123  Why yes I will totally submit my photos to a s...           1.0\n",
       "8124  Test on a Saturday! Thank you uni! #sarcasm @ ...           1.0\n",
       "8125  Listening to 's Misery isn't at all disconcert...           1.0\n",
       "8126   There you go being kind again #sarcasm #stand...           1.0\n",
       "8127  I'm shocked that these refs in the tcu vs minn...           1.0\n",
       "\n",
       "[100108 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging two datasets\n",
    "\n",
    "sarcasm_df = pd.concat([data_df,twitter_df])\n",
    "sarcasm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcasm_df.to_csv('sarcasm_df.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100108"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sarcasm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "max_length = 32\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sarcasm_df['sentence'].tolist())\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sentence_sequences = tokenizer.texts_to_sequences(sarcasm_df['sentence'].tolist())\n",
    "sentence_padded = pad_sequences(sentence_sequences, maxlen = max_length, padding = padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   18,    39,  2126, ...,     0,     0,     0],\n",
       "       [  282,   135,  1809, ...,     0,     0,     0],\n",
       "       [ 1289,     9,     2, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [27748,   893,  3879, ...,     0,     0,     0],\n",
       "       [  160,  6019,  1895, ...,     0,     0,     0],\n",
       "       [ 1730,  2298,     6, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/jupyter/6/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sentence_padded = np.array(sentence_padded)\n",
    "print(type(sentence_padded))\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "#sarcasm_df = sarcasm_df.drop(['sentence_padded','tokenizd_sentence'],axis=1)\n",
    "sarcasm_df['tokenized_sentence'] = sparse.coo_matrix(sentence_padded).toarray().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Default Embedding, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index)\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "\n",
    "# Defining Early stopping to stop training if validation accuracy does not improve within five epochs\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "    #tf.keras.layers.Dropout(0.2),\n",
    "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences = True)),\n",
    "    #tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    #tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 100)           8374600   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 32, 256)           234496    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 256)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32, 128)           32896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32, 1)             129       \n",
      "=================================================================\n",
      "Total params: 8,642,121\n",
      "Trainable params: 8,642,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66528</th>\n",
       "      <td>it was obvious they wanted more size at WR wh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[18, 39, 2146, 49, 683, 68, 2439, 23, 10395, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70824</th>\n",
       "      <td>I've been guilty of posting #sarcastic notes l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[284, 135, 1814, 8, 2183, 96, 3963, 45, 167, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38987</th>\n",
       "      <td>#DonaldTrump in the middle of the stage, other...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1297, 9, 2, 605, 8, 2, 1072, 179, 689, 663, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4180</th>\n",
       "      <td>There is a 100 chance of Pickleman’s Pizza bei...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[109, 11, 6, 698, 985, 8, 81832, 1227, 95, 510...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10276</th>\n",
       "      <td>a look at transgender sex workers living in china</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[6, 175, 23, 2218, 391, 1268, 579, 9, 768, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18117</th>\n",
       "      <td>bush not heard from for over a month</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[528, 27, 584, 32, 13, 108, 6, 735, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23165</th>\n",
       "      <td>Pregnant Chicken NAILS IT!  #parenthood #irony...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1710, 1395, 4329, 18, 1226, 7, 5, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48717</th>\n",
       "      <td>1990s: UN bodies on #drugs amp #crime merge to...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[32535, 893, 3875, 14, 58, 60, 1057, 8106, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16869</th>\n",
       "      <td>gop promotes carly fiorina to male candidate a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[160, 5878, 1889, 1428, 3, 1450, 691, 83, 1157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21528</th>\n",
       "      <td>Huh.   sends a lot of \"we really care!\" emails...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1730, 2307, 6, 371, 8, 46, 92, 367, 1367, 212...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100108 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  is_sarcastic  \\\n",
       "66528   it was obvious they wanted more size at WR wh...           1.0   \n",
       "70824  I've been guilty of posting #sarcastic notes l...           1.0   \n",
       "38987  #DonaldTrump in the middle of the stage, other...           0.0   \n",
       "4180   There is a 100 chance of Pickleman’s Pizza bei...           0.0   \n",
       "10276  a look at transgender sex workers living in china           0.0   \n",
       "...                                                  ...           ...   \n",
       "18117               bush not heard from for over a month           1.0   \n",
       "23165  Pregnant Chicken NAILS IT!  #parenthood #irony...           0.0   \n",
       "48717  1990s: UN bodies on #drugs amp #crime merge to...           0.0   \n",
       "16869  gop promotes carly fiorina to male candidate a...           1.0   \n",
       "21528  Huh.   sends a lot of \"we really care!\" emails...           0.0   \n",
       "\n",
       "                                      tokenized_sentence  \n",
       "66528  [18, 39, 2146, 49, 683, 68, 2439, 23, 10395, 4...  \n",
       "70824  [284, 135, 1814, 8, 2183, 96, 3963, 45, 167, 1...  \n",
       "38987  [1297, 9, 2, 605, 8, 2, 1072, 179, 689, 663, 1...  \n",
       "4180   [109, 11, 6, 698, 985, 8, 81832, 1227, 95, 510...  \n",
       "10276  [6, 175, 23, 2218, 391, 1268, 579, 9, 768, 0, ...  \n",
       "...                                                  ...  \n",
       "18117  [528, 27, 584, 32, 13, 108, 6, 735, 0, 0, 0, 0...  \n",
       "23165  [1710, 1395, 4329, 18, 1226, 7, 5, 0, 0, 0, 0,...  \n",
       "48717  [32535, 893, 3875, 14, 58, 60, 1057, 8106, 3, ...  \n",
       "16869  [160, 5878, 1889, 1428, 3, 1450, 691, 83, 1157...  \n",
       "21528  [1730, 2307, 6, 371, 8, 46, 92, 367, 1367, 212...  \n",
       "\n",
       "[100108 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_df = sarcasm_df.sample(frac=1)\n",
    "sarcasm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcasm_df = sarcasm_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      1      2 ... 100095 100096 100097] [     4      9     31 ... 100083 100098 100099]\n",
      "704/704 [==============================] - 165s 228ms/step - loss: 0.4103 - accuracy: 0.7893 - val_loss: 0.2899 - val_accuracy: 0.8432\n",
      "[     0      1      2 ... 100097 100098 100099] [     8     12     27 ... 100047 100084 100093]\n",
      "704/704 [==============================] - 171s 243ms/step - loss: 0.2495 - accuracy: 0.8655 - val_loss: 0.2498 - val_accuracy: 0.8641\n",
      "[     0      1      2 ... 100097 100098 100099] [    16     23     25 ... 100080 100082 100095]\n",
      "704/704 [==============================] - 183s 261ms/step - loss: 0.2200 - accuracy: 0.8816 - val_loss: 0.2105 - val_accuracy: 0.8876\n",
      "[     2      3      4 ... 100097 100098 100099] [     0      1      5 ... 100074 100089 100096]\n",
      "704/704 [==============================] - 199s 283ms/step - loss: 0.1757 - accuracy: 0.9128 - val_loss: 0.1797 - val_accuracy: 0.9085\n",
      "[     0      1      2 ... 100097 100098 100099] [    13     29     42 ... 100070 100079 100091]\n",
      "704/704 [==============================] - 158s 224ms/step - loss: 0.1317 - accuracy: 0.9380 - val_loss: 0.1205 - val_accuracy: 0.9418\n",
      "[     0      1      2 ... 100097 100098 100099] [    11     48     72 ... 100069 100087 100090]\n",
      "704/704 [==============================] - 155s 220ms/step - loss: 0.0988 - accuracy: 0.9544 - val_loss: 0.0959 - val_accuracy: 0.9559\n",
      "[     0      1      4 ... 100097 100098 100099] [     2      3     10 ... 100088 100092 100094]\n",
      "704/704 [==============================] - 151s 215ms/step - loss: 0.0768 - accuracy: 0.9650 - val_loss: 0.0672 - val_accuracy: 0.9676\n",
      "[     0      1      2 ... 100096 100098 100099] [    17     20     24 ... 100072 100086 100097]\n",
      "704/704 [==============================] - 149s 212ms/step - loss: 0.0615 - accuracy: 0.9720 - val_loss: 0.0603 - val_accuracy: 0.9727\n",
      "[     0      1      2 ... 100097 100098 100099] [    14     15     21 ... 100065 100066 100081]\n",
      "704/704 [==============================] - 216s 308ms/step - loss: 0.0532 - accuracy: 0.9757 - val_loss: 0.0449 - val_accuracy: 0.9793\n",
      "[     0      1      2 ... 100097 100098 100099] [     7     53     77 ... 100053 100067 100085]\n",
      "704/704 [==============================] - 184s 261ms/step - loss: 0.0444 - accuracy: 0.9798 - val_loss: 0.0397 - val_accuracy: 0.9816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "k = 10\n",
    "\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "X = sarcasm_df.iloc[:,-1]\n",
    "y = sarcasm_df['is_sarcastic'].tolist()\n",
    "\n",
    "X = np.array(X.tolist())\n",
    "y = np.array([int(i) for i in y])\n",
    "\n",
    "for train_index, test_index in folds.split(X,y):\n",
    "    print(train_index,test_index)\n",
    "    \n",
    "    X_train, X_test = X[train_index],X[test_index]\n",
    "    y_train, y_test = y[train_index] , y[test_index]\n",
    "    \n",
    "    model.fit(X_train,y_train,batch_size = 128,validation_data=(X_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
